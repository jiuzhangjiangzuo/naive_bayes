{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a78549649effa1858818cefd9f614338590f14c"
   },
   "source": [
    "# 垃圾邮件分类\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6b1ab40-34da-4645-98c4-4b61c97895f6",
    "_uuid": "0259bd59e4fad41ffd78aa5a510a83ef07daab7f",
    "collapsed": true
   },
   "source": [
    "拿到数据首先读入拿到数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a89760f4-9075-42be-b4c4-c8c87d175c88",
    "_uuid": "4429a082ddabcb23261daec29ee1ae9e68ba2a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拆分过后的每个邮件内容\n",
      "1114    No no:)this is kallis home ground.amla home to...\n",
      "3589    I am in escape theatre now. . Going to watch K...\n",
      "3095    We walked from my moms. Right on stagwood pass...\n",
      "1012       I dunno they close oredi not... ÌÏ v ma fan...\n",
      "3320                               Yo im right by yo work\n",
      "4130    \\Its Ur luck to Love someone. Its Ur fortune t...\n",
      "1197     He also knows about lunch menu only da. . I know\n",
      "5426        Oh yeah! And my diet just flew out the window\n",
      "624     Nah it's straight, if you can just bring bud o...\n",
      "2260    SplashMobile: Choose from 1000s of gr8 tones e...\n",
      "Name: v2, dtype: object\n",
      "拆分过后每个邮件是否是垃圾邮件\n",
      "1114     ham\n",
      "3589     ham\n",
      "3095     ham\n",
      "1012     ham\n",
      "3320     ham\n",
      "4130     ham\n",
      "1197     ham\n",
      "5426     ham\n",
      "624      ham\n",
      "2260    spam\n",
      "Name: v1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "data_dir = \"../input/\"\n",
    "df = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')\n",
    "\n",
    "# 把数据拆分成为训练集和测试集\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    df.v2,\n",
    "    df.v1, \n",
    "    test_size=0.2, \n",
    "    random_state=0)  \n",
    "\n",
    "print ('拆分过后的每个邮件内容')\n",
    "print (data_train[:10])\n",
    "print ('拆分过后每个邮件是否是垃圾邮件')\n",
    "print (labels_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d7250a2a135c1e13ccfc00f6848176ecee31ff8"
   },
   "source": [
    "建立词汇表，统计两个类目下面的共词计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "f7954ab7449b37d9d68880e7e56ab6f35630a8a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all the unique words : 11706\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    用一个dictionary保存词汇，并给每个词汇赋予唯一的id\n",
    "'''\n",
    "def GetVocabulary(data): \n",
    "    vocab_dict = {}\n",
    "    wid = 0\n",
    "    for document in data:\n",
    "        words = document.split() #按空格分词 “I am a student” => [\"I\", \"am\", \"a\", \"student\"]\n",
    "        for word in words:\n",
    "            word = word.lower() #归一化\n",
    "            if word not in vocab_dict:\n",
    "                vocab_dict[word] = wid\n",
    "                wid += 1\n",
    "    return vocab_dict\n",
    "\n",
    "# 用训练集建立词汇表\n",
    "vocab_dict = GetVocabulary(data_train)\n",
    "print ('Number of all the unique words : ' + str(len(vocab_dict.keys())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bb58cf7bf7b6e8fe37e89d4fc6a4edba21dc62d"
   },
   "source": [
    "把文章变成词向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "db14dd202a4cb2bab6306d14e3acd2cb151f1260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "1.0 1.0 2.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    把文本变成向量的表示形式，以便进行计算\n",
    "'''\n",
    "def Document2Vector(vocab_dict, data):\n",
    "    word_vector = np.zeros(len(vocab_dict.keys()))\n",
    "    words = data.split()\n",
    "    out_of_voc = 0\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in vocab_dict:\n",
    "            word_vector[vocab_dict[word]] += 1\n",
    "        else:\n",
    "            out_of_voc += 1\n",
    "    return word_vector, out_of_voc\n",
    "\n",
    "# 下面是一个例子，解释向量长什么样\n",
    "example, _ = Document2Vector(vocab_dict,\"we are good good\")\n",
    "print(example)\n",
    "print(example[vocab_dict['we']], example[vocab_dict['are']], example[vocab_dict['good']])\n",
    "# 每个单词是一个维度，如果单词没有出现过，对应那一维为0，否则为出现的次数.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f8e259bb2ef260294478c4ec7929cdd10b68d84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457\n"
     ]
    }
   ],
   "source": [
    "# 把训练集的句子全部变成向量形式\n",
    "train_matrix = []\n",
    "for document in data_train.values:\n",
    "    word_vector, _ = Document2Vector(vocab_dict, document)\n",
    "    train_matrix.append(word_vector)\n",
    "\n",
    "print (len(train_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b55b0ce4caf250c26efb4f543d257744ef8c1985"
   },
   "source": [
    "做naive bayes 训练，得到训练集每个词概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "024ad891e7df647a1415d9106e42cf8951f91f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on the doc id:0\n",
      "Train on the doc id:500\n",
      "Train on the doc id:1000\n",
      "Train on the doc id:1500\n",
      "Train on the doc id:2000\n",
      "Train on the doc id:2500\n",
      "Train on the doc id:3000\n",
      "Train on the doc id:3500\n",
      "Train on the doc id:4000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    在训练集计算两种概率：\n",
    "        1. 词在每个分类下的概率，比如P('email'|Spam)\n",
    "        2. 每个分类的概率，比如P(Spam)\n",
    "        \n",
    "    这里的计算实现巧妙利用了numpy的array结构：\n",
    "        1. 在每个分类下创建一个与词汇量大小相等的vector(即 numpy array), 即spam_word_counter 和 ham_word_counter\n",
    "        2. 在遍历每一个句子的时候，直接与句子对应的vector相加，累积每个单词出现的次数\n",
    "        3. 在遍历完所有句子之后，再除以总词汇量，得到每个单词的概率\n",
    "'''\n",
    "def NaiveBayes_train(train_matrix,labels_train):\n",
    "    # train_matrix => (10，1000)\n",
    "    num_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0]) #对第一个样本去一下vector的长度\n",
    "    \n",
    "    spam_word_counter = np.ones(num_words)\n",
    "    ham_word_counter = np.ones(num_words)  #计算频数初始化为1，即使用拉普拉斯平滑\n",
    "\n",
    "    ham_total_count = 0;\n",
    "    spam_total_count = 0;\n",
    "    \n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    for i in range(num_docs):\n",
    "        if i % 500 == 0:\n",
    "            print ('Train on the doc id:' + str(i))\n",
    "            \n",
    "        if labels_train[i] == 'ham':\n",
    "            ham_word_counter += train_matrix[i]\n",
    "            ham_total_count += sum(train_matrix[i])\n",
    "            ham_count += 1\n",
    "        else:\n",
    "            spam_word_counter += train_matrix[i]\n",
    "            spam_total_count += sum(train_matrix[i])\n",
    "            spam_count += 1\n",
    "    \n",
    "    #spam_word_counter => 每个词的计数\n",
    "    #spam_total_count => Spam的总次数\n",
    "    #spam_count => Spam邮件计数\n",
    "    \n",
    "    # 注意，这里对所有的概率都取了log\n",
    "    p_spam_vector = np.log(spam_word_counter/(spam_total_count + num_words)) #注意在分母也加上平滑部分\n",
    "    p_ham_vector = np.log(ham_word_counter/(ham_total_count + num_words))  #注意在分母也加上平滑部分\n",
    "    \n",
    "    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs), spam_total_count, ham_total_count\n",
    "\n",
    "# p_spam_vector/p_ham_vector 的每一维分别是一个单词在spam/ham分类下的概率\n",
    "# p_spam / p_ham 分别是两个分类的概率\n",
    "p_spam_vector, p_spam, p_ham_vector, p_ham, spam_total_count, ham_total_count = NaiveBayes_train(train_matrix, labels_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行测试集预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "e4415b37-0855-49bd-8926-afe63c668268",
    "_uuid": "1d737630d3f2df2b3d28355f57cabbf594c2bf23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on the doc id:0\n",
      "Test on the doc id:200\n",
      "Test on the doc id:400\n",
      "Test on the doc id:600\n",
      "Test on the doc id:800\n",
      "Test on the doc id:1000\n",
      "1115\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    对测试集进行预测，按照公式计算例子在两个分类下的概率，选择概率较大者作为预测结果\n",
    "'''\n",
    "def Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham, spam_smoothing, ham_smoothing):\n",
    "    \n",
    "    # 注意: 如果单词没出现过，则test_word_vector对应的维度为0\n",
    "    # 所以: test_word_vector * p_spam_vector 不为0的维度正好是句子中每个词的概率\n",
    "    # [2, 0, 1] * [0.3, 0.2, 0.4] = sum([0.6, 0, 0.4]) = \n",
    "    # \n",
    "    spam = sum(test_word_vector * p_spam_vector) + p_spam + spam_smoothing\n",
    "    ham = sum(test_word_vector * p_ham_vector) + p_ham + ham_smoothing\n",
    "    if spam > ham:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "num_words = len(vocab_dict.keys())\n",
    "predictions = []\n",
    "i = 0\n",
    "for document in data_test.values:\n",
    "    if i % 200 == 0:\n",
    "        print ('Test on the doc id:' + str(i))\n",
    "    i += 1    \n",
    "    test_word_vector, out_of_voc = Document2Vector(vocab_dict, document)\n",
    "    # Add smoothing for out_of_vocbulary words\n",
    "    if out_of_voc != 0:\n",
    "        spam_smoothing = np.log(out_of_voc/(spam_total_count + num_words))\n",
    "        ham_smoothing = np.log(out_of_voc/(ham_total_count + num_words))\n",
    "    else:\n",
    "        spam_smoothing = 0\n",
    "        ham_smoothing = 0\n",
    "    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham, spam_smoothing, ham_smoothing)\n",
    "    predictions.append(ans)\n",
    "\n",
    "print (len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "231f268550db0c2515b227e89d8f3510ce1d26c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978475336323\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99       949\n",
      "       spam       0.98      0.87      0.92       166\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1115\n",
      "\n",
      "[[946   3]\n",
      " [ 21 145]]\n"
     ]
    }
   ],
   "source": [
    "# 检测模型\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print (accuracy_score(labels_test, predictions))\n",
    "print (classification_report(labels_test, predictions))\n",
    "print (confusion_matrix(labels_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
